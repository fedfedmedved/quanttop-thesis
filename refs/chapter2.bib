@misc{Ulyanov2016,
author = {Ulyanov, Dmitry},
booktitle = {GitHub repository},
publisher = {GitHub},
title = {{Multicore-TSNE}},
url = {https://github.com/DmitryUlyanov/Multicore-TSNE},
year = {2016}
}
@article{Andrecut2008,
abstract = {Principal component analysis (PCA) is a key statistical technique for multivariate data analysis. For large data sets the common approach to PCA computation is based on the standard NIPALS-PCA algorithm, which unfortunately suffers from loss of orthogonality, and therefore its applicability is usually limited to the estimation of the first few components. Here we present an algorithm based on Gram-Schmidt orthogonalization (called GS-PCA), which eliminates this shortcoming of NIPALS-PCA. Also, we discuss the GPU (Graphics Processing Unit) parallel implementation of both NIPALS-PCA and GS-PCA algorithms. The numerical results show that the GPU parallel optimized versions, based on CUBLAS (NVIDIA) are substantially faster (up to 12 times) than the CPU optimized versions based on CBLAS (GNU Scientific Library).},
archivePrefix = {arXiv},
arxivId = {0811.1081},
author = {Andrecut, M.},
eprint = {0811.1081},
file = {:home/p3732/Downloads/Parallel GPU Implementation of Iterative PCA Algorithms (0811.1081).pdf:pdf},
title = {{Parallel GPU Implementation of Iterative PCA Algorithms}},
year = {2008}
}
@article{sne,
author = {Hinton, G.E. and Roweis, S.T.},
file = {:home/p3732/MA/papers/SNE (2276-stochastic-neighbor-embedding).pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {833--840},
title = {{Stochastic Neighbor Embedding}},
url = {http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf},
volume = {15},
year = {2002}
}
@article{Gotz2015,
author = {G{\"{o}}tz, Markus and Bodenstein, Christian and Riedel, Morris},
doi = {10.1145/2834892.2834894},
file = {:home/p3732/MA/papers/HPDBSCAN (paper).pdf:pdf},
title = {{HPDBSCAN – Highly Parallel DBSCAN}},
year = {2015}
}
@article{Jolliffe2002,
abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jolliffe, I T},
doi = {10.1007/b98835},
eprint = {arXiv:1011.1669v3},
file = {:home/p3732/MA/papers/Jolliffe I. Principal Component Analysis (2ed., Springer, 2002)(518s){\_}MVsa{\_}.pdf:pdf},
isbn = {0-387-95442-2},
issn = {01621459},
journal = {Springer Series in Statistics},
keywords = {principal component analysis,statistical theory methods},
pages = {487},
pmid = {21674720},
title = {{Principal Component Analysis, Second Edition}},
url = {http://link.springer.com/10.1007/b98835},
volume = {98},
year = {2002}
}
@inproceedings{Arthur2007,
author = {Arthur, David and Vassilvitskii, Sergei},
booktitle = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
pages = {1027--1035},
publisher = {Society for Industrial and Applied Mathematics Philadelphia},
title = {{k-means++: The Advantages of Careful Seeding}},
url = {http://portal.acm.org/citation.cfm?id=1283494},
year = {2007}
}
@book{Manning2012,
author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"{u}}tze, Hinrich},
doi = {10.1017/CBO9780511809071},
file = {:home/p3732/Downloads/irbookonlinereading.pdf:pdf},
isbn = {9780511809071},
publisher = {Cambridge University Press},
title = {{Introduction to Information Retrieval}},
year = {2012}
}
@article{Heer2010,
abstract = {A survey of powerful visualization techniques, from the obvious to the obscure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Heer, Jeffrey and Bostock, Michael and Ogievetsky, Vadim},
doi = {10.1145/1743546},
eprint = {arXiv:1011.1669v3},
file = {:home/p3732/MA/papers/A Tour Through the Visualization Zoo (p59-heer).pdf:pdf},
isbn = {00010782},
issn = {00010782},
journal = {Communications of the ACM},
number = {5},
pages = {59--67},
pmid = {15224215},
title = {{A Tour through the Visualization Zoo}},
volume = {53},
year = {2010}
}
@inproceedings{dbscan,
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"{o}}rg and Xu, Xiaowei},
booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
doi = {10.1.1.121.9220},
file = {:home/p3732/MA/papers/dbscan (10.1.1.121.9220).pdf:pdf},
pages = {226----231},
publisher = {AAAI Press},
title = {{A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise}},
year = {1996}
}
@article{Gan2015,
abstract = {DBSCAN is a popular method for clustering multi-dimensional objects. Just as notable as the method's vast success is the research community's quest for its efficient computation. The original KDD'96 paper claimed an algorithm with O(n log n) running time, where n is the number of objects. Unfortunately, this is a mis-claim; and that algorithm actually requires O(n 2) time. There has been a fix in 2D space, where a genuine O(n log n)-time algorithm has been found. Looking for a fix for dimensionality d ≥ 3 is currently an important open problem. In this paper, we prove that for d ≥ 3, the DBSCAN problem requires Ω(n 4/3) time to solve, unless very significant breakthroughs—ones widely believed to be impossible—could be made in theoretical computer science. This (i) explains why the community's search for fixing the aforementioned mis-claim has been futile for d ≥ 3, and (ii) indicates (sadly) that all DBSCAN algorithms must be intolerably slow even on moderately large n in practice. Surprisingly, we show that the running time can be dramatically brought down to O(n) in expectation regardless of the dimensionality d, as soon as slight inaccuracy in the clustering results is permitted. We formalize our findings into the new notion of $\rho$-approximate DBSCAN, which we believe should replace DBSCAN on big data due to the latter's computational intractability.},
author = {Gan, Junhao and Tao, Yufei},
doi = {10.1145/2723372.2737792},
file = {:home/p3732/MA/papers/sigmod15-dbscan.pdf:pdf},
isbn = {9781450327589},
journal = {Sigmod},
keywords = {10,4 snake-shaped clusters,algorithm,dbscan,density-based clustering,examples of density-based clustering,figure 1,from,the left one contains,while the right one},
pages = {519--530},
title = {{DBSCAN Revisited: Mis-Claim, Un-Fixablility, and Approxmimation}},
year = {2015}
}
@article{Aloise2009,
abstract = {A recent proof of NP-hardness of Euclidean sum-of-squares clustering, due to Drineas et al. (Mach. Learn. 56:9–33, 2004), is not valid. An alternate short proof is pro-vided.},
author = {Aloise, Daniel and Deshpande, Amit and Hansen, Pierre and Popat, Preyas},
doi = {10.1007/s10994-009-5103-0},
file = {:home/p3732/MA/papers/Aloise2009{\_}Article{\_}NP-hardnessOfEuclideanSum-of-s.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Clustering,Complexity,Sum-of-squares},
number = {2},
pages = {245--248},
title = {{NP-hardness of Euclidean sum-of-squares clustering}},
volume = {75},
year = {2009}
}
@article{rptrees,
author = {Dasgupta, Sanjoy and Freund, Yoav},
file = {:home/p3732/MA/papers/RPtree (rptree-stoc).pdf:pdf},
title = {{Random Projection Trees and Low Dimensional Manifolds}},
url = {http://cseweb.ucsd.edu/{~}dasgupta/papers/rptree-stoc.pdf},
year = {2008}
}
@inproceedings{Dong2011,
author = {Dong, Wei and Charikar, Moses and Li, Kai},
booktitle = {Proceedings of the 20th international conference on World wide web},
doi = {10.1145/1963405.1963487},
file = {:home/p3732/MA/papers/NNdescent (www11).pdf:pdf},
keywords = {arbitrary similarity measure,iter-,k -nearest neighbor graph},
pages = {577--586},
title = {{Efficient K-Nearest Neighbor Graph Construction for Generic Similarity Measures}},
year = {2011}
}
