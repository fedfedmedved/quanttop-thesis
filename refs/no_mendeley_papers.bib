@article{faiss,
archivePrefix = {arXiv},
arxivId = {1702.08734},
author = {Johnson, Jeff and Douze, Matthijs and J{\'{e}}gou, Herv{\'{e}}},
eprint = {1702.08734},
file = {:home/pet/MA/papers/Billion-scale similarity search with GPUs (1702.08734).pdf:pdf},
title = {{Billion-scale similarity search with GPUs}},
year = {2017}
}
@article{mnist,
author = {Lecun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
file = {:home/pet/MA/papers/MNIST (Convolution{\_}nets).pdf:pdf},
keywords = {character recognition,convolutional neural networks,document recog-,finite state transducers,gradient-based learning,graph,machine learning,neural networks,nition,ocr,optical,transformer networks},
number = {11},
pages = {2278--2324},
title = {{Gradient-Based Learning Applied to Document Recognition}},
url = {http://www.doi.org/10.1109/5.726791},
volume = {86},
year = {1998}
}
@article{umap,
abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP as described has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
archivePrefix = {arXiv},
arxivId = {1802.03426},
author = {McInnes, Leland and Healy, John},
doi = {10.21105/joss.00861},
eprint = {1802.03426},
file = {:home/pet/MA/papers/UMAP$\backslash$: Uniform Manifold Approximation and Projection for Dimension Reduction (1802.03426).pdf:pdf},
issn = {2475-9066},
pages = {1--18},
title = {{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}},
url = {http://www.doi.org/10.21105/joss.00861},
year = {2018}
}
@article{Burtscher2011,
abstract = {This chapter describes the first CUDA implementation of the classical Barnes Hut n-body algorithm that runs entirely on the GPU. The Barnes Hut force-calculation algorithm is widely used in n-body simulations such as modeling the motion of galaxies. It hierarchically decomposes the space around the bodies into successively smaller boxes, called cells, and computes summary information for the bodies contained in each cell, allowing the algorithm to quickly approximate the forces (e.g., gravitational, electric, or magnetic) that the n bodies induce upon each other. The Barnes Hut algorithm is challenging to implement efficiently in CUDA because it repeatedly builds and traverses an irregular tree-based data structure, it performs a lot of pointer-chasing memory operations, and it is typically expressed recursively. GPU-specific operations such as thread-voting functions are exploited to greatly improve performance and make use of fence instructions to implement lightweight synchronization without atomic operations. The main conclusion of the work is that GPUs can be used to accelerate irregular codes, not just regular codes. However, a great deal of programming effort is required to achieve good performance. The biggest performance win, though, came from turning some of the unique architectural features of GPUs, which are often regarded as performance hurdles for irregular codes, into assets. Future direction indicates workings on writing high-performing CUDA implementations for other irregular algorithms. {\textcopyright} 2011 Copyright {\textcopyright} 2011 NVIDIA Corporation and Wen-mei W. Hwu Published by Elsevier Inc. All rights reserved..},
author = {Burtscher, Martin and Pingali, Keshav},
doi = {10.1016/B978-0-12-384988-5.00006-1},
file = {:home/pet/MA/new papers/burtscher11.pdf:pdf},
isbn = {9780123849885},
journal = {GPU Computing Gems Emerald Edition},
pages = {75--92},
title = {{An Efficient CUDA Implementation of the Tree-Based Barnes Hut n-Body Algorithm}},
year = {2011}
}
@article{tsne-cuda,
abstract = {Modern datasets and models are notoriously difficult to explore and analyze due to their inherent high dimensionality and massive numbers of samples. Existing visualization methods which employ dimensionality reduction to two or three dimensions are often inefficient and/or ineffective for these datasets. This paper introduces t-SNE-CUDA, a GPU-accelerated implementation of t-distributed Symmetric Neighbor Embedding (t-SNE) for visualizing datasets and models. t-SNE-CUDA significantly outperforms current implementations with 50-700x speedups on the CIFAR-10 and MNIST datasets. These speedups enable, for the first time, visualization of the neural network activations on the entire ImageNet dataset - a feat that was previously computationally intractable. We also demonstrate visualization performance in the NLP domain by visualizing the GloVe embedding vectors. From these visualizations, we can draw interesting conclusions about using the L2 metric in these embedding spaces. t-SNE-CUDA is publicly available athttps://github.com/CannyLab/tsne-cuda},
archivePrefix = {arXiv},
arxivId = {1807.11824},
author = {Chan, David M. and Rao, Roshan and Huang, Forrest and Canny, John F.},
eprint = {1807.11824},
file = {:home/pet/MA/papers/t-SNE-CUDA$\backslash$: GPU-Accelerated t-SNE and its Applications to Modern Data (1807.11824).pdf:pdf},
title = {{t-SNE-CUDA: GPU-Accelerated t-SNE and its Applications to Modern Data}},
url = {http://arxiv.org/abs/1807.11824},
year = {2018}
}
@article{Barnes1986,
author = {Barnes, Josh and Hut, Piet},
doi = {10.1038/324446a0},
file = {:home/pet/MA/papers/A hierarchical O(N log N) force-calculation algorithm (324446a0).pdf:pdf},
journal = {Nature},
month = {dec},
pages = {446},
publisher = {Nature Publishing Group},
title = {{A Hierarchical O(N log N) Force-Calculation Algorithm}},
volume = {324},
year = {1986}
}
@article{Lloyd1982,
abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for quanta, , are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
author = {Lloyd, Stuart P.},
doi = {10.1109/TIT.1982.1056489},
file = {:home/pet/MA/papers/Least Squares Quantization in PCM (lloyd57).pdf:pdf},
isbn = {00189448 (ISSN)},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
number = {2},
pages = {129--137},
title = {{Least Squares Quantization in PCM}},
volume = {28},
year = {1982}
}
@article{Pezzotti2018,
abstract = {The t-distributed Stochastic Neighbor Embedding (tSNE) algorithm has become in recent years one of the most used and insightful techniques for the exploratory data analysis of high-dimensional data. tSNE reveals clusters of high-dimensional data points at different scales while it requires only minimal tuning of its parameters. Despite these advantages, the computational complexity of the algorithm limits its application to relatively small datasets. To address this problem, several evolutions of tSNE have been developed in recent years, mainly focusing on the scalability of the similarity computations between data points. However, these contributions are insufficient to achieve interactive rates when visualizing the evolution of the tSNE embedding for large datasets. In this work, we present a novel approach to the minimization of the tSNE objective function that heavily relies on modern graphics hardware and has linear computational complexity. Our technique does not only beat the state of the art, but can even be executed on the client side in a browser. We propose to approximate the repulsion forces between data points using adaptive-resolution textures that are drawn at every iteration with WebGL. This approximation allows us to reformulate the tSNE minimization problem as a series of tensor operation that are computed with TensorFlow.js, a JavaScript library for scalable tensor computations.},
archivePrefix = {arXiv},
arxivId = {1805.10817},
author = {Pezzotti, Nicola and Mordvintsev, Alexander and Hollt, Thomas and Lelieveldt, Boudewijn P. F. and Eisemann, Elmar and Vilanova, Anna},
eprint = {1805.10817},
file = {:home/pet/MA/papers/Linear tSNE Optimization for the Web (1805.10817v1).pdf:pdf},
pages = {1--6},
title = {{Linear tSNE Optimization for the Web}},
year = {2018}
}
@article{tsne,
abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence theway in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
author = {{Van Der Maaten}, Laurens and Hinton, Geoffrey},
file = {:home/pet/MA/papers/Visualizing high-dimensional data using t-sne (JMLR{\_}2008).pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
title = {{Visualizing High-Dimensional Data Using t-SNE}},
url = {http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf},
volume = {9},
year = {2008}
}
@article{bhtsne,
abstract = {The paper investigates the acceleration of t-SNE—an embedding technique that is com- monly used for the visualization of high-dimensional data in scatter plots—using two tree- based algorithms. In particular, the paper develops variants of the Barnes-Hut algorithm and of the dual-tree algorithm that approximate the gradient used for learning t-SNE em- beddings in O(N logN). Our experiments show that the resulting algorithms substantially accelerate t-SNE, and that they make it possible to learn embeddings of data sets with millions of objects. Somewhat counterintuitively, the Barnes-Hut variant of t-SNE appears to outperform the dual-tree variant.},
author = {{Van Der Maaten}, Laurens},
file = {:home/pet/MA/papers/Accelerating t-SNE using Tree-Based Algorithms (JMLR{\_}2014).pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {barnes-hut algorithm,dual-tree algorithm,embedding,multidimensional scaling,space-partitioning trees,t-sne},
number = {2014},
pages = {3221--3245},
title = {{Accelerating t-SNE using Tree-Based Algorithms}},
url = {http://jmlr.org/papers/v15/vandermaaten14a.html},
volume = {15},
year = {2014}
}
