# Methods {#methods}

## Profiling UMAP
To efficiently parallelize an algorithm, it is important to know how well the individual steps of the algorithm perform.
Parallelizing a part that hardly contributes to the algorithm's running time, will not result in a noticeable speedup.
By profiling the algorithm, the parts that take longest to execute can be found.
Parallelizing these parts will have the biggest impact on the performance of the algorithm.

UMAP is a general dimensionality reduction algorithm and is thus applicable to a multitude of data sets.
For the following profiling a wide selection of data sets is used, in order to capture how the performance varies for different inputs.
The data sets are:
the Iris flower data set [@iris],
the Pen Digits data set [@digits],
the COIL-20 [@coil20] and COIL-100 [@coil100] data sets,
the Labeled Faces in the Wild (LFW) data set [@lfw],
the MNIST data set [@mnist],
the Fashion-MNIST data set [@fashion-mnist],
the CIFAR-10 data set [@cifar10] and
the GoogleNews word vectors data set [@googlenews].
For each data set the number of samples $N$ and dimensionality $M$ is given in the profiling table.
The majority of these data sets are also used to measure algorithm performances in the original UMAP publication [@umap].
This allows for a validation of the publication's claimed execution times.

Since this thesis focuses on using UMAP for visualization, all data sets are reduced to two dimensions.
All UMAP input parameters are set to the implementation's standard values, including $K = 15$ for the KNN search and an iteration count of $T = 500$ for data sets with $N < 10000$, or $T = 300$ otherwise.
The standard Python profiler [^pyprof] is used for accurate timing.
It measures the time spent on each function of the program.
This enables the identification of program parts that take long to execute and therefore are good candidates for parallelization.

All profiling is performed on a system with an Intel® Core™ i7-7700 CPU with 8 cores and 16 GB of main memory.
<!--The system is also equipped with an Nvidia® GeForce GTX™ 745 GPU.-->
UMAP in version 0.3.8 is used along with Numba version 0.43.1.
The operating system is Archlinux with kernel version 5.0.7 and Python 3.7.3.

@fig:tuna-mnist shows a visualization of profiling data generated by processing the MNIST data set.
The graphic is taken from the interactive output that was generated by the `tuna` Python package[^tuna_src] for the profiling data.
As can be seen, the total runtime is dominated by few functions.
The percentages in the graphic include loading times of data sets and libraries and are therefore too low, but nonetheless more than half of the time is spent in the `simplicial_set_embedding` method.
It performs the initialization and optimization of the low-dimensional representation (Algorithm \autoref{umap_algo}, line 7-11).
Other significant contributions are made by the `nearest_neighbors` method, which performs a KNN search (Algorithm \autoref{umap_algo}, line 2-6), and the `fuzzy_simplicial_set` method, which normalizes the distances between data points found by the KNN search (as illustrated in @fig:umap_radii).

![Profiling of UMAP on the MNIST data set.](figures/chapter3/tuna_mnist.png){#fig:tuna-mnist short-caption="Profiling UMAP on the MNIST data set." width="100%"}

In the following table the total time to process each data set is given, along with a breakdown into time spent on each of the dominating methods: `simplicial_set_embedding` (Embedding), `nearest_neighbors` (KNN) and `fuzzy_simplicial_set` (Fuzzy).
UMAP uses Numba [@numba], a Just-In-Time (JIT) compiler.
It compiles source code "on the fly" during execution of the program.
Consequently the time taken for these compilations also accounts for parts of the run time.
This time is not prominently present in @fig:tuna-mnist, since compiling is done at several occasions during the execution.
The accumulated time taken for all compilations is given in an additional column (JIT).

<!--regex to create table from .dat data file-->
<!--\n([^ \n]+) ([^\n ]+) ([^ \n]+) ([^ \n]+) ([^\n ]+) ([^ \n]+) ([^ \n]+) ([^ \n]+) [^\n]+ \n-->
<!--\n|\1|\2|\3|\5|\7|\6|\8|\4|\n-->
|Data Set|$N$|$M$|Total Time|KNN|Fuzzy|Embedding|JIT|
|----|--:|--:|----:|---:|---:|---:|---:|
|Iris|150|4|5.585|0.001|3.406|0.233|5.207|
|Coil-20|1440|16384|10.107|0.069|3.525|2.143|5.256|
|Pen Digits|1797|64|8.376|0.131|3.553|2.899|5.194|
|LFW|13233|2914|27.510|11.044|3.947|10.093|7.234|
|COIL-20|7000|16384|31.318|13.537|3.689|10.913|7.338|
|MNIST|70000|784|85.118|19.837|6.503|51.878|7.234|
|Fashion-MNIST|70000|784|90.547|22.416|6.562|54.470|7.308|
|CIFAR|60000|3072|121.415|56.669|6.191|51.582|7.308|
|GoogleNews|200000|300|311.362|87.260|12.572|177.772|7.096|
|GoogleNews|500000|300|950.920|274.285|29.234|444.736|12.053|

The times were measured by loading a data set, preprocessing it if necessary and performing UMAP.
The scripts for profiling and parsing the profiling data can be found in the repository of this thesis [^repo_thesis].
Both profilings of the GoogleNews data set were performed on a down sampled subset, since the original data set has two million data points and did not fit in the system's main memory.

All data sets were made contiguous in the main memory, meaning that they were placed consecutively in memory without any fragmentation.
Initial test profilings did not assure the data to be contiguous, which resulted in implausibly high execution times.
In these test runs a visualization of the MNIST data was performed in 436.5 seconds, 5 times slower than the contiguous version.
A processing of the CIFAR-10 data set even took 3033 seconds, 25 times as long as the revised running time.
With contiguous data the measured times align with those of the UMAP publication.

To estimate the influence of the data set's number of samples and dimensions on the running time, a separate profiling is performed.
UMAP is hereby applied to reduced versions of the CIFAR data set.
A reduction is made in equidistant steps for both, the data set size and dimensionality.
The size is reduced in steps of 10.000 data points, the dimensionality is reduced in steps of 25% of the original number of dimensions.
<!--This is done by dropping rows columns-->
CIFAR was chosen, because it is big enough to allow for the creation of meaningful subsets through reduction, while still being small enough to be processed repetitively in a feasible time frame.

![Profiling of UMAP on varying sizes and dimensions of CIFAR.](figures/chapter3/plot_umap_cifar_total.png){#fig:prof_n_and_m_cifar width="100%"}

As the visualized profiling results of @fig:prof_n_and_m_cifar show, the running time of UMAP is influenced stronger by the amount of samples, than by the dimensionality.
Processing only half of all data points reduces the running time more, than processing the same amount of data with only a fourth of the dimensions.
@fig:prof_n_and_m_methods shows how this is reflected in the individual methods of UMAP.
It shows the contribution of each method that resulted in the total running time of @fig:prof_n_and_m_cifar.

<div id="fig:prof_n_and_m_methods" class="subfigures">
![`nearest_neighbors`](figures/chapter3/plot_umap_cifar_NN.png){width=45% #fig:prof_n_and_m_methods_a}\hfill
![`fuzzy_simplicial_set`](figures/chapter3/plot_umap_cifar_fuzzy_set.png){width=45% #fig:prof_n_and_m_methods_b}

![`simplicial_set_embedding`](figures/chapter3/plot_umap_cifar_embed.png){width=45% #fig:prof_n_and_m_methods_c}\hfill
![JIT compiling](figures/chapter3/plot_umap_cifar_compile.png){width=45% #fig:prof_n_and_m_methods_d}

Time spent on individual methods while processing reduced CIFAR data sets.
</div>

<!--for different sizes of CIFAR, -->

JIT compiling always takes the same amount of time and is not affected by the input data.
Similarly, `fuzzy_simplicial_set` stays almost constant, indicating only little growth with the data set size.
The real differences in total stem from the `nearest_neighbors` and `simplicial_set_embedding` methods.
Both show a linear growth.
The KNN search does depend on size and dimensionality of the input data, whereas the layout optimization works with the low-dimensional representation created by UMAP and only is affected by the size and number of dimensions to reduce to.

A clearer comparison of these times is given in @fig:prof_n_and_m_2d_cifar.
For it, solely the data set size has been altered, while the original dimensionality was kept.
An analogous breakdown for a profiling of MNIST with the same range of data set size, shows how the approximately four times higher dimensionality of CIFAR affects the running time.

<div id="fig:prof_n_and_m_2d" class="subfigures">
![MNIST](figures/chapter3/plot_2d_mnist.png){width=49% #fig:prof_n_and_m_2d_mnist}\hfill
![CIFAR](figures/chapter3/plot_2d_cifar.png){width=49% #fig:prof_n_and_m_2d_cifar}

Profiling of UMAP methods for reduced CIFAR and MNIST data sets.
</div>

Finally, a profiling to analyze how deterministic UMAP behaves:
For @fig:determinism UMAP was performed on subsets of the CIFAR data set a total of 10 times.
The variation of the execution time is visualized by displaying the span between the fastest and the slowest time.
However, in the graphic the error cannot be seen, since it is small in comparison to the total time.
This leads to the conclusion, that the runtime of UMAP is rather deterministic.
<!--preprocess with pca to same dimensions does not have any influence on the performance-->

<div id="fig:determinism" class="subfigures">
![MNIST](figures/chapter3/deterministic_mnist.png){width=49% #fig:determinism_mnist}\hfill
![CIFAR](figures/chapter3/deterministic_cifar.png){width=49% #fig:determinism_cifar}

Minimum and maximum execution times of UMAP on variations of the CIFAR and MNIST data sets.
</div>

